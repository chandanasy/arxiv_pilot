# -*- coding: utf-8 -*-
"""metadata_df.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110pEp3B-cFF1y3fmrGubFhC0GbpN7ybh
"""

import pandas as pd

df1 = pd.read_csv("/content/paper_references.csv")

df1

!pip install feedparser

import requests
import feedparser

# Define the base URL of the arXiv API
ARXIV_API_URL = "http://export.arxiv.org/api/query?"

# Parameters for the API query
query_params = {
    "search_query": "cat:quant-ph",  # Search for articles in the Quantum Physics category
    "start": 0,  # Starting index (for pagination)
    "max_results": 5  # Number of results to return
}

# Make the GET request to the arXiv API
response = requests.get(ARXIV_API_URL, params=query_params)

# Parse the response using feedparser
feed = feedparser.parse(response.content)

# Loop through each entry and print out metadata
for entry in feed.entries:
    print(f"Title: {entry.title}")
    print(f"Authors: {', '.join(author.name for author in entry.authors)}")
    print(f"Published: {entry.published}")
    print(f"Abstract: {entry.summary}")
    print(f"arXiv ID: {entry.id.split('/abs/')[-1]}")
    print("-" * 80)

import requests
import feedparser
import pandas as pd

# Example DataFrame
data = {
    'paper': ['arxiv_pdfs/2403.07865v2.pdf', 'arxiv_pdfs/2403.07872v1.pdf'],
    'refers': ['arXiv:2403.07865', 'arXiv:2403.07872, arXiv:2303.08774, arXiv:2311...']
}
df1 = pd.DataFrame(data)

# Extract arXiv IDs from the 'paper' column
df1['arxiv_id'] = df1['paper'].str.extract(r'(\d+\.\d+)')

# Define the base URL of the arXiv API
ARXIV_API_URL = "http://export.arxiv.org/api/query?"

for arxiv_id in df1['arxiv_id'].unique():
    # Make sure we have a valid arXiv ID before proceeding
    if pd.notna(arxiv_id):
        # Parameters for the API query
        query_params = {
            "id_list": arxiv_id
        }

        # Make the GET request to the arXiv API
        response = requests.get(ARXIV_API_URL, params=query_params)

        # Parse the response using feedparser
        feed = feedparser.parse(response.content)

        # Loop through each entry and print out metadata
        for entry in feed.entries:
            print(f"Title: {entry.title}")
            print(f"Authors: {', '.join(author.name for author in entry.authors)}")
            print(f"Published: {entry.published}")
            print(f"Abstract: {entry.summary}")
            print(f"arXiv ID: {entry.id.split('/abs/')[-1]}")
            print("-" * 80)

df1['arxiv_id'] = df1['paper'].str.extract(r'(\d+\.\d+)')

print(df1)

import requests
import feedparser

def fetch_metadata(arxiv_id):
    ARXIV_API_URL = "http://export.arxiv.org/api/query"
    query_params = {
        'id_list': arxiv_id
    }
    response = requests.get(ARXIV_API_URL, params=query_params)
    feed = feedparser.parse(response.content)
    if feed.entries:
        entry = feed.entries[0]
        metadata = {
            'Title': entry.title,
            'Authors': ', '.join(author.name for author in entry.authors),
            'Published': entry.published,
            'Abstract': entry.summary,
            'arXiv ID': entry.id.split('/abs/')[-1]
        }
        return metadata
    else:
        return None

# Iterate through the DataFrame and fetch metadata for each arXiv ID
for _, row in df1.iterrows():
    arxiv_id = row['arxiv_id']
    metadata = fetch_metadata(arxiv_id)
    if metadata:
        print(f"Metadata for arXiv ID {arxiv_id}:")
        for key, value in metadata.items():
            print(f"{key}: {value}")
        print("-" * 80)
    else:
        print(f"No metadata found for arXiv ID {arxiv_id}")

def fetch_metadata(arxiv_id):
    ARXIV_API_URL = "http://export.arxiv.org/api/query"
    query_params = {
        'id_list': arxiv_id
    }
    response = requests.get(ARXIV_API_URL, params=query_params)
    feed = feedparser.parse(response.content)
    if feed.entries:
        entry = feed.entries[0]
        metadata = {
            'arXiv ID': entry.id.split('/abs/')[-1],
            'Authors': ', '.join(author.name for author in entry.authors),
            'Published': entry.published,
            'Abstract': entry.summary
        }
        return metadata
    else:
        return None

# Assuming 'df' is your DataFrame and it has a column 'arxiv_id'
metadata_list = []

for _, row in df1.iterrows():
    arxiv_id = row['arxiv_id']
    metadata = fetch_metadata(arxiv_id)
    if metadata:
        metadata_list.append(metadata)
    else:
        print(f"No metadata found for arXiv ID {arxiv_id}")

metadata_df = pd.DataFrame(metadata_list)
metadata_df.to_csv('arxiv_metadata.csv', index=False)

from google.colab import files

# Assuming 'df' is your DataFrame and you want to save it as 'paper_references.csv'
# Use files.download to prompt your browser to download the file to your local machine
files.download('arxiv_metadata.csv')

